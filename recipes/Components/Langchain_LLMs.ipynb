{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLMs with Langchain\n",
    "\n",
    "In this recipe, we show how to obtain a model client for different providers using Langchain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install git+https://github.com/ibm-granite-community/utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Replicate\n",
    "\n",
    "Granite models are available in the [`ibm-granite`](https://replicate.com/ibm-granite) org at Replicate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.llms import Replicate\n",
    "from ibm_granite_community.notebook_utils import get_env_var\n",
    "\n",
    "model = Replicate(\n",
    "    model=\"ibm-granite/granite-8b-code-instruct-128k\",\n",
    "    replicate_api_token=get_env_var('REPLICATE_API_TOKEN'),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ollama\n",
    "\n",
    "Granite Code models are hosted in the [Ollama granite code library](https://ollama.com/library/granite-code). To use Ollama to host Granite models you need to first install the Ollama server locally on your system or in the notebook itself.\n",
    "\n",
    "#### Install Ollama Server Locally\n",
    "\n",
    "1. [Download and install Ollama](https://ollama.com/download), if you haven't already.\n",
    "1. Start the Ollama server: `ollama serve`\n",
    "\n",
    "#### Install Ollama Server in the Notebook\n",
    "\n",
    "Following \"Option B: Shell Script (Linux)\" in [Getting Started with Ollama](https://github.com/ibm-granite-community/utils/blob/main/recipes/Getting_Started/Getting_Started_with_Ollama.ipynb), run the following commands in separate notebook cells:\n",
    "\n",
    "1. Install Ollama\n",
    "2. Start the Ollama server\n",
    "\n",
    "A Granite model can then be served using the code cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama.llms import OllamaLLM\n",
    "\n",
    "model = OllamaLLM(model=\"granite-code:8b\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WatsonX\n",
    "\n",
    "The Granite models available on WatsonX are listed [here](https://www.ibm.com/products/watsonx-ai/foundation-models)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ibm import WatsonxLLM\n",
    "from ibm_granite_community.notebook_utils import get_env_var\n",
    "\n",
    "model = WatsonxLLM(\n",
    "    model_id=\"ibm/granite-8b-code-instruct\",\n",
    "    url= get_env_var(\"WATSONX_URL\"),\n",
    "    apikey=get_env_var(\"WATSONX_APIKEY\"),\n",
    "    project_id=get_env_var(\"WATSONX_PROJECT_ID\"),\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
