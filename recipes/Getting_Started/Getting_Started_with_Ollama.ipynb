{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting Started with Ollama"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running an LLM on Ollama "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install Ollama\n",
    "\n",
    "_Option A: Package Installer (MacOS, Windows)_\n",
    "\n",
    "[Download from Ollama.com](https://ollama.com/download) and install "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Option B: Shell Script (Linux)_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl https://ollama.ai/install.sh | sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start the Ollama server\n",
    "\n",
    "Using `nohup` and `&` will run the server in the background and you can close the terminal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.system(\"nohup ollama serve &\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pull the model\n",
    "\n",
    "IBM Granite Code models available on Ollama can be found [here](https://ollama.com/library/granite-code).\n",
    "\n",
    "Granite Code 3b: `ollama pull granite-code:3b`\n",
    "\n",
    "Granite Code 8b: `ollama pull granite-code:8b`\n",
    "\n",
    "Granite Code 20b: `ollama pull granite-code:20b`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ollama pull granite-code:3b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Querying the LLM with Langchain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select a model\n",
    "\n",
    "Select one of the models you pulled with Ollama above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"granite-code:3b\"\n",
    "# model_id = \"granite-code:8b\"\n",
    "# model_id = \"granite-code:20b\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instantiate the model client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama.llms import OllamaLLM\n",
    "\n",
    "model = OllamaLLM(model=model_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform inference\n",
    "\n",
    "Invoke the model with a prompt, and get an answer back."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"\n",
    "    Show me a SQL query that fetches all columns for the first 50 rows\n",
    "    in a table named 'users'.\"\"\"\n",
    "\n",
    "response = model.invoke(prompt)\n",
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
